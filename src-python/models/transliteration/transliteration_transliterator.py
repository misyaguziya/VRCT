from sudachipy import tokenizer
from sudachipy import dictionary
try:
    from .transliteration_kana_to_hepburn import katakana_to_hepburn
except ImportError:
    from transliteration_kana_to_hepburn import katakana_to_hepburn

class Transliterator:
    def __init__(self):
        self.tokenizer_obj = dictionary.Dictionary(dict_type="full").create()
        self.mode = tokenizer.Tokenizer.SplitMode.C

    @staticmethod
    def is_kanji(ch: str) -> bool:
        return '\u4e00' <= ch <= '\u9fff'

    @staticmethod
    def kata_to_hira(text: str) -> str:
        return "".join(
            chr(ord(c) - 0x60) if 'ã‚¡' <= c <= 'ãƒ³' else c
            for c in text
        )

    @staticmethod
    def split_kanji_okurigana(surface: str, reading_kana: str, use_macron: bool = True):
        """
        1èªã®è¡¨å±¤å½¢(surface)ã¨èª­ã¿(reading_kana)ã‚’
        [ {"orig":..., "kana":..., "hira":..., "hepburn":...}, ... ] ã«åˆ†å‰²
        """
        result = []

        # è¡¨å±¤ã‚’ã€Œæ¼¢å­—ãƒ–ãƒ­ãƒƒã‚¯ã€ã¨ã€Œéæ¼¢å­—ãƒ–ãƒ­ãƒƒã‚¯ã€ã«åˆ†å‰²
        buf = ""
        prev_is_kanji = None
        blocks = []
        for ch in surface:
            now_is_kanji = Transliterator.is_kanji(ch)
            if prev_is_kanji is None or now_is_kanji == prev_is_kanji:
                buf += ch
            else:
                blocks.append((prev_is_kanji, buf))
                buf = ch
            prev_is_kanji = now_is_kanji
        if buf:
            blocks.append((prev_is_kanji, buf))

        # èª­ã¿ã‚’åˆ†é…
        kana_left = reading_kana
        for i, (is_kan, part) in enumerate(blocks):
            if is_kan:
                # æ¼¢å­—ãƒ–ãƒ­ãƒƒã‚¯ã®å‡¦ç†
                if len(blocks) == 1:
                    # å˜ä¸€ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆå…¨ã¦æ¼¢å­—ï¼‰ã®å ´åˆ
                    kana_for_kan = kana_left
                elif i == len(blocks) - 1:
                    # æœ€å¾Œã®ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆæ¼¢å­—ï¼‰ã®å ´åˆ
                    kana_for_kan = kana_left
                else:
                    # ä¸­é–“ã®æ¼¢å­—ãƒ–ãƒ­ãƒƒã‚¯ã®å ´åˆ
                    # å¾Œç¶šã®éæ¼¢å­—ãƒ–ãƒ­ãƒƒã‚¯ã®æ–‡å­—æ•°ã‚’è¨ˆç®—
                    remaining_non_kanji = sum(len(p) for is_k, p in blocks[i+1:] if not is_k)
                    if remaining_non_kanji > 0 and len(kana_left) > remaining_non_kanji:
                        kana_for_kan = kana_left[:-remaining_non_kanji]
                    else:
                        # æ¼¢å­—1æ–‡å­—ã‚ãŸã‚Šæœ€ä½1æ–‡å­—ã®èª­ã¿ã‚’å‰²ã‚Šå½“ã¦
                        min_kana = len(part)
                        kana_for_kan = kana_left[:max(min_kana, len(kana_left) - remaining_non_kanji)]
                
                # ç©ºã®èª­ã¿ã‚’é¿ã‘ã‚‹
                if not kana_for_kan and kana_left:
                    kana_for_kan = kana_left[:1]

                result.append({
                    "orig": part,
                    "kana": kana_for_kan,
                    "hira": Transliterator.kata_to_hira(kana_for_kan),
                    "hepburn": katakana_to_hepburn(kana_for_kan, use_macron=use_macron)
                })
                kana_left = kana_left[len(kana_for_kan):]
            else:
                # éæ¼¢å­—éƒ¨åˆ†ï¼ˆé€ã‚Šä»®åãªã©ï¼‰
                kana_for_okuri = kana_left[:len(part)]
                result.append(
                    {
                        "orig": part,
                        "kana": kana_for_okuri,
                        "hira": Transliterator.kata_to_hira(kana_for_okuri),
                        "hepburn": katakana_to_hepburn(kana_for_okuri, use_macron=use_macron)
                    }
                )
                kana_left = kana_left[len(kana_for_okuri):]

        return result

    def analyze(self, text: str, use_macron: bool = False):
        tokens = self.tokenizer_obj.tokenize(text, self.mode)

        results = []
        for t in tokens:
            surface = t.surface()
            reading = t.reading_form()
            pos = t.part_of_speech()

            if pos and pos[0] in ["è¨˜å·", "è£œåŠ©è¨˜å·", "ç©ºç™½"]:
                reading = surface

            if surface == reading:
                results.append({
                    "orig": surface,
                    "kana": reading,
                    "hira": surface,
                    "hepburn": surface,
                })
                continue

            # å˜ç´”ã«1æ–‡å­—ãšã¤å‡¦ç†
            if len(surface) == 1:
                # 1æ–‡å­—ã®å ´åˆã¯ãã®ã¾ã¾
                results.append({
                    "orig": surface,
                    "kana": reading,
                    "hira": self.kata_to_hira(reading),
                    "hepburn": katakana_to_hepburn(reading, use_macron=use_macron)
                })
            else:
                # è¤‡æ•°æ–‡å­—ã®å ´åˆã¯æ—¢å­˜ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã§åˆ†å‰²
                parts = self.split_kanji_okurigana(surface, reading, use_macron=use_macron)
                results.extend(parts)

        return results

# --- ãƒ†ã‚¹ãƒˆ ---
if __name__ == "__main__":
    test_cases = [
        "ç¾ã—ã„èŠ±ã‚’è¦‹ã‚‹",
        "æ±äº¬ã«è¡Œã",
        "æ¼¢å­—ã¨ã‚«ã‚¿ã‚«ãƒŠã®æ··åœ¨",
        "ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼ã«è¡Œã",
        "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ã†",
        "ã‚·ã‚§ã‚¢ãƒã‚¦ã‚¹ã«ä½ã‚€",
        "ãƒ´ã‚¡ã‚¤ã‚ªãƒªãƒ³ã‚’å¼¾ã",
        "ã‚®ãƒ¥ã‚¦ãƒ‹ãƒ¥ã‚¦ã‚’é£²ã‚€",
        "ãƒ‹ãƒ¥ãƒ¼ãƒ¨ãƒ¼ã‚¯ã«è¡Œã",
        "ãƒ©ãƒ¼ãƒ¡ãƒ³ã‚’é£Ÿã¹ã‚‹",
        "ãƒãƒ§ã‚³ãƒ¬ãƒ¼ãƒˆãŒå¥½ã",
        "SessionIDã‚’å–å¾—ã™ã‚‹",
        "å–ã‚Šæ•¢ãˆãšæ¤œç´¢ã—ã¦ã¿ã‚‹",
        "è¦‹çŸ¥ã‚‰ã¬åœŸåœ°ã§å†’é™ºã™ã‚‹",
        "å½¼ã¯å„ªã‚ŒãŸã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™",
        " ".join(list("[]<>!@#$%^&*()_+-={}|\;:'\",.<>/?`~")),
        " ".join(list("ã€Œã€ï¼œï¼ï¼ï¼ ï¼ƒï¼„ï¼…ï¼¾ï¼†ï¼Šï¼ˆï¼‰ï¼¿ï¼‹ï¼ï¼ï½›ï½ï½œï¼¼ï¼›ï¼šï¼‡ï¼‚ï¼Œï¼ï¼ï¼Ÿï½€ï½")),
        " ".join(list("â™ªâ™«â™¬â™­â™®â™¯Â°â„ƒâ„‰â„–â„«Â®Â©â„¢âœ“âœ”âœ•âœ–â˜…â˜†â—‹â—â—â—‡â—†â–¡â– â–³â–²â–½â–¼â€»â†’â†â†‘â†“â†”ï¸â†•ï¸â‡„â‡…âˆâˆ´âˆµâˆ·â‰ªâ‰«â‰¦â‰§Â±Ã—Ã·â‰ â‰ˆâ‰¡âŠ‚âŠƒâŠ†âŠ‡âŠ„âŠ…âˆªâˆ©âˆˆâˆ‹âˆ…âˆ€âˆƒâˆ âŠ¥âŒ’âˆ‚âˆ‡âˆšâˆ«âˆ¬âˆ®âˆ‘âˆâˆ§âˆ¨Â¬â‡’â‡”âˆ€âˆƒâˆ âŠ¥âŒ’âˆ‚âˆ‡âˆšâˆ«âˆ¬âˆ®âˆ‘âˆ")),
        " ".join(list("ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ˜…ğŸ˜‚ğŸ¤£ğŸ˜ŠğŸ˜‡ğŸ™‚"))
    ]

    transliterator = Transliterator()
    for case in test_cases:
        print(transliterator.analyze(case))
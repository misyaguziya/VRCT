from sudachipy import tokenizer
from sudachipy import dictionary
try:
    from .transliteration_kana_to_hepburn import katakana_to_hepburn
except ImportError:
    from transliteration_kana_to_hepburn import katakana_to_hepburn
try:
    from .transliteration_context_rules import apply_context_rules
except ImportError:
    from transliteration_context_rules import apply_context_rules

class Transliterator:
    def __init__(self):
        self.tokenizer_obj = dictionary.Dictionary(dict_type="full").create()
        self.mode = tokenizer.Tokenizer.SplitMode.C

    @staticmethod
    def is_kanji(ch: str) -> bool:
        return '\u4e00' <= ch <= '\u9fff'

    @staticmethod
    def kata_to_hira(text: str) -> str:
        return "".join(
            chr(ord(c) - 0x60) if 'ã‚¡' <= c <= 'ãƒ³' else c
            for c in text
        )

    @staticmethod
    def split_kanji_okurigana(surface: str, reading_kana: str, use_macron: bool = True):
        """Split a single surface word and its kana reading into parts.

        Inputs:
        - surface: the surface form (may contain kanji + kana)
        - reading_kana: the katakana reading for the whole surface

        Output:
        - a list of dicts: [{"orig": str, "kana": str, "hira": str, "hepburn": str}, ...]

        Notes:
        - The function allocates portions of ``reading_kana`` to each contiguous
          kanji/non-kanji block in ``surface``. Allocation is heuristic: an
          initial allocation based on block length is used and any remainder is
          distributed left-to-right preferring kanji blocks.
        - This function is pure (no external side effects) and returns the
          constructed list.
        """

        result = []

        # è¡¨å±¤ã‚’ã€Œæ¼¢å­—ãƒ–ãƒ­ãƒƒã‚¯ã€ã¨ã€Œéæ¼¢å­—ãƒ–ãƒ­ãƒƒã‚¯ã€ã«åˆ†å‰²
        buf = ""
        prev_is_kanji = None
        blocks = []
        for ch in surface:
            now_is_kanji = Transliterator.is_kanji(ch)
            if prev_is_kanji is None or now_is_kanji == prev_is_kanji:
                buf += ch
            else:
                blocks.append((prev_is_kanji, buf))
                buf = ch
            prev_is_kanji = now_is_kanji
        if buf:
            blocks.append((prev_is_kanji, buf))

        # èª­ã¿ã‚’åˆ†é…
        kana_left = reading_kana
        # We'll allocate kana to each block by initial guess = len(part) (characters)
        # and distribute any remaining kana left-to-right preferring kanji blocks.
        kana_len = len(kana_left)

        # initial allocation per block
        allocs = [len(part) for _, part in blocks]
        allocated = sum(allocs)
        remaining = kana_len - allocated

        # distribute extra kana to kanji blocks first (left-to-right)
        if remaining > 0:
            for idx, (is_kan, _) in enumerate(blocks):
                if remaining <= 0:
                    break
                if is_kan:
                    allocs[idx] += 1
                    remaining -= 1
            # if still remaining, distribute to all blocks left-to-right
            idx = 0
            while remaining > 0 and len(blocks) > 0:
                allocs[idx] += 1
                remaining -= 1
                idx = (idx + 1) % len(blocks)

        # if remaining < 0 (reading shorter than base), shrink allocations from right
        if remaining < 0:
            # remove from rightmost blocks as needed
            need = -remaining
            idx = len(blocks) - 1
            while need > 0 and idx >= 0:
                take = min(allocs[idx] - 1, need) if allocs[idx] > 1 else 0
                allocs[idx] -= take
                need -= take
                idx -= 1

        # now slice kana_left according to allocs
        pos = 0
        for (is_kan, part), cnt in zip(blocks, allocs):
            kana_for_part = kana_left[pos:pos+cnt]
            pos += cnt
            result.append({
                "orig": part,
                "kana": kana_for_part,
                "hira": Transliterator.kata_to_hira(kana_for_part),
                "hepburn": katakana_to_hepburn(kana_for_part, use_macron=use_macron)
            })

        return result

    def analyze(self, text: str, use_macron: bool = False):
        """Tokenize ``text`` and produce per-subunit reading information.

        Returns a list of dicts for each token/sub-part with keys:
        - orig: original surface string (one or more characters)
        - kana: katakana reading for this part (may be adapted by context rules)
        - hira: hiragana reading (derived from kana)
        - hepburn: Latin transcription (derived from kana)

        Side-effects / notes:
        - The function calls ``apply_context_rules(results, use_macron=...)``
          which both mutates ``results`` in-place and returns it. This method
          safely accepts the returned list and then recalculates ``hira`` and
          ``hepburn`` for entries whose ``kana`` was changed.
        - If rule application fails, analysis still returns the best-effort
          results.
        """

        tokens = self.tokenizer_obj.tokenize(text, self.mode)

        results = []
        for t in tokens:
            surface = t.surface()
            reading = t.reading_form()
            pos = t.part_of_speech()

            if pos and pos[0] in ["è¨˜å·", "è£œåŠ©è¨˜å·", "ç©ºç™½"]:
                reading = surface

            if surface == reading:
                results.append({
                    "orig": surface,
                    "kana": reading,
                    "hira": surface,
                    "hepburn": surface,
                })
                continue

            # å˜ç´”ã«1æ–‡å­—ãšã¤å‡¦ç†
            if len(surface) == 1:
                # 1æ–‡å­—ã®å ´åˆã¯ãã®ã¾ã¾
                results.append({
                    "orig": surface,
                    "kana": reading,
                    "hira": self.kata_to_hira(reading),
                    "hepburn": katakana_to_hepburn(reading, use_macron=use_macron)
                })
            else:
                # è¤‡æ•°æ–‡å­—ã®å ´åˆã¯æ—¢å­˜ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã§åˆ†å‰²
                parts = self.split_kanji_okurigana(surface, reading, use_macron=use_macron)
                results.extend(parts)

        # æ–‡è„ˆãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨ï¼ˆåˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
        try:
            results = apply_context_rules(results, use_macron=use_macron) or results
        except Exception:
            # ãƒ«ãƒ¼ãƒ«é©ç”¨ã§å¤±æ•—ã—ã¦ã‚‚è§£æçµæœã¯è¿”ã™
            pass

        # apply_context_rules ãŒ kana ã‚’æ›¸ãæ›ãˆãŸå ´åˆã€hira ã¨ hepburn ã‚’å†è¨ˆç®—
        for entry in results:
            kana = entry.get("kana", "")
            if kana:
                entry["hira"] = self.kata_to_hira(kana)
                entry["hepburn"] = katakana_to_hepburn(kana, use_macron=use_macron)

        return results

# --- ãƒ†ã‚¹ãƒˆ ---
if __name__ == "__main__":
    import pprint
    test_cases = [
        "å‘ã“ã†ã¸è¡Œã",
        "è¡Œäº‹ã‚’è¡Œã†",
        "ä¸ŠãŒã‚‹",
        "ä¸Šã‚‹",
        "å…¥ã‚Šè¾¼ã‚€",
        "ä½•",
        "ä½•ãŒå¥½ãï¼Ÿ",
        "ä½•è‰²ãŒå¥½ãï¼Ÿ",
        "ä½•è‰²ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "ä½•èªã§ã™ã‹ï¼Ÿ",
        "ãƒ†ãƒ¼ãƒ–ãƒ«ã«è‰²é‰›ç­†ã¯ä½•è‰²ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
        "ç¾ã—ã„èŠ±ã‚’è¦‹ã‚‹",
        "æ±äº¬ã«è¡Œã",
        "æ¼¢å­—ã¨ã‚«ã‚¿ã‚«ãƒŠã®æ··åœ¨",
        "ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼ã«è¡Œã",
        "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ã†",
        "ã‚·ã‚§ã‚¢ãƒã‚¦ã‚¹ã«ä½ã‚€",
        "ãƒ´ã‚¡ã‚¤ã‚ªãƒªãƒ³ã‚’å¼¾ã",
        "ã‚®ãƒ¥ã‚¦ãƒ‹ãƒ¥ã‚¦ã‚’é£²ã‚€",
        "ãƒ‹ãƒ¥ãƒ¼ãƒ¨ãƒ¼ã‚¯ã«è¡Œã",
        "ãƒ©ãƒ¼ãƒ¡ãƒ³ã‚’é£Ÿã¹ã‚‹",
        "ãƒãƒ§ã‚³ãƒ¬ãƒ¼ãƒˆãŒå¥½ã",
        "SessionIDã‚’å–å¾—ã™ã‚‹",
        "å–ã‚Šæ•¢ãˆãšæ¤œç´¢ã—ã¦ã¿ã‚‹",
        "è¦‹çŸ¥ã‚‰ã¬åœŸåœ°ã§å†’é™ºã™ã‚‹",
        "å½¼ã¯å„ªã‚ŒãŸã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™",
        " ".join(list("[]<>!@#$%^&*()_+-={}|\;:'\",.<>/?`~")),
        " ".join(list("ã€Œã€ï¼œï¼ï¼ï¼ ï¼ƒï¼„ï¼…ï¼¾ï¼†ï¼Šï¼ˆï¼‰ï¼¿ï¼‹ï¼ï¼ï½›ï½ï½œï¼¼ï¼›ï¼šï¼‡ï¼‚ï¼Œï¼ï¼ï¼Ÿï½€ï½")),
        " ".join(list("â™ªâ™«â™¬â™­â™®â™¯Â°â„ƒâ„‰â„–â„«Â®Â©â„¢âœ“âœ”âœ•âœ–â˜…â˜†â—‹â—â—â—‡â—†â–¡â– â–³â–²â–½â–¼â€»â†’â†â†‘â†“â†”ï¸â†•ï¸â‡„â‡…âˆâˆ´âˆµâˆ·â‰ªâ‰«â‰¦â‰§Â±Ã—Ã·â‰ â‰ˆâ‰¡âŠ‚âŠƒâŠ†âŠ‡âŠ„âŠ…âˆªâˆ©âˆˆâˆ‹âˆ…âˆ€âˆƒâˆ âŠ¥âŒ’âˆ‚âˆ‡âˆšâˆ«âˆ¬âˆ®âˆ‘âˆâˆ§âˆ¨Â¬â‡’â‡”âˆ€âˆƒâˆ âŠ¥âŒ’âˆ‚âˆ‡âˆšâˆ«âˆ¬âˆ®âˆ‘âˆ")),
        " ".join(list("ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ˜…ğŸ˜‚ğŸ¤£ğŸ˜ŠğŸ˜‡ğŸ™‚"))
    ]

    transliterator = Transliterator()
    for case in test_cases:
        pprint.pprint(transliterator.analyze(case), sort_dicts=False)
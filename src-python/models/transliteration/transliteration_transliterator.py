import spacy

try:
    from .transliteration_kana_to_hepburn import katakana_to_hepburn
except ImportError:
    from transliteration_kana_to_hepburn import katakana_to_hepburn

class Transliterator:
    def __init__(self):
        self.ja_ginza_nlp = spacy.load('ja_ginza')

    @staticmethod
    def is_kanji(ch: str) -> bool:
        return '\u4e00' <= ch <= '\u9fff'

    @staticmethod
    def kata_to_hira(text: str) -> str:
        if text is None:
            return ""
        # token.morph.get ãªã©ã‹ã‚‰ list/tuple ãŒæ¥ã‚‹å ´åˆãŒã‚ã‚‹ã®ã§æ­£è¦åŒ–ã™ã‚‹
        if isinstance(text, (list, tuple)):
            text = text[0] if text else ""
        # å®‰å…¨ã®ãŸã‚æ–‡å­—åˆ—åŒ–
        text = str(text)
        out_chars = []
        for c in text:
            # ord() ã¯é•·ã•1ã®æ–‡å­—åˆ—ã‚’æœŸå¾…ã™ã‚‹ã®ã§é•·ã•1ã®ã¨ãã®ã¿å‡¦ç†ã™ã‚‹
            if len(c) == 1 and 'ã‚¡' <= c <= 'ãƒ³':
                out_chars.append(chr(ord(c) - 0x60))
            else:
                out_chars.append(c)
        return "".join(out_chars)

    @staticmethod
    def split_kanji_okurigana(surface: str, reading_kana: str, use_macron: bool = False):
        """
        1èªã®è¡¨å±¤å½¢(surface)ã¨èª­ã¿(reading_kana)ã‚’
        [ {"orig":..., "kana":..., "hira":..., "hepburn":...}, ... ] ã«åˆ†å‰²
        """
        result = []

        # è¡¨å±¤ã‚’ã€Œæ¼¢å­—ãƒ–ãƒ­ãƒƒã‚¯ã€ã¨ã€Œéæ¼¢å­—ãƒ–ãƒ­ãƒƒã‚¯ã€ã«åˆ†å‰²
        buf = ""
        prev_is_kanji = None
        blocks = []
        for ch in surface:
            now_is_kanji = Transliterator.is_kanji(ch)
            if prev_is_kanji is None or now_is_kanji == prev_is_kanji:
                buf += ch
            else:
                blocks.append((prev_is_kanji, buf))
                buf = ch
            prev_is_kanji = now_is_kanji
        if buf:
            blocks.append((prev_is_kanji, buf))

        # èª­ã¿ã‚’åˆ†é…
        kana_left = reading_kana
        for i, (is_kan, part) in enumerate(blocks):
            if is_kan:
                # æ¼¢å­—ãƒ–ãƒ­ãƒƒã‚¯ã®å‡¦ç†
                if len(blocks) == 1:
                    # å˜ä¸€ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆå…¨ã¦æ¼¢å­—ï¼‰ã®å ´åˆ
                    kana_for_kan = kana_left
                elif i == len(blocks) - 1:
                    # æœ€å¾Œã®ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆæ¼¢å­—ï¼‰ã®å ´åˆ
                    kana_for_kan = kana_left
                else:
                    # ä¸­é–“ã®æ¼¢å­—ãƒ–ãƒ­ãƒƒã‚¯ã®å ´åˆ
                    # å¾Œç¶šã®éæ¼¢å­—ãƒ–ãƒ­ãƒƒã‚¯ã®æ–‡å­—æ•°ã‚’è¨ˆç®—
                    remaining_non_kanji = sum(len(p) for is_k, p in blocks[i+1:] if not is_k)
                    if remaining_non_kanji > 0 and len(kana_left) > remaining_non_kanji:
                        kana_for_kan = kana_left[:-remaining_non_kanji]
                    else:
                        # æ¼¢å­—1æ–‡å­—ã‚ãŸã‚Šæœ€ä½1æ–‡å­—ã®èª­ã¿ã‚’å‰²ã‚Šå½“ã¦
                        min_kana = len(part)
                        kana_for_kan = kana_left[:max(min_kana, len(kana_left) - remaining_non_kanji)]
                
                # ç©ºã®èª­ã¿ã‚’é¿ã‘ã‚‹
                if not kana_for_kan and kana_left:
                    kana_for_kan = kana_left[:1]
                
                result.append(
                    {
                        "orig": part,
                        "kana": kana_for_kan,
                        "hira": Transliterator.kata_to_hira(kana_for_kan),
                        "hepburn": katakana_to_hepburn(kana_for_kan, use_macron=use_macron)
                    }
                )
                kana_left = kana_left[len(kana_for_kan):]
            else:
                # éæ¼¢å­—éƒ¨åˆ†ï¼ˆé€ã‚Šä»®åãªã©ï¼‰
                kana_for_okuri = kana_left[:len(part)]
                result.append(
                    {
                        "orig": part,
                        "kana": kana_for_okuri,
                        "hira": Transliterator.kata_to_hira(kana_for_okuri),
                        "hepburn": katakana_to_hepburn(kana_for_okuri, use_macron=use_macron)
                    }
                )
                kana_left = kana_left[len(kana_for_okuri):]

        return result

    def analyze(self, text: str, use_macron: bool = False):
        doc = self.ja_ginza_nlp(text)

        results = []
        for sent in doc.sents:
            for token in sent:
                surface = token.text
                # token.morph.get("Reading") ã¯ list/tuple/None ã‚’è¿”ã™å ´åˆãŒã‚ã‚‹ãŸã‚æ­£è¦åŒ–
                reading_raw = token.morph.get("Reading")
                if isinstance(reading_raw, (list, tuple)):
                    reading = reading_raw[0] if reading_raw else None
                else:
                    reading = reading_raw
                # èª­ã¿ãŒå¾—ã‚‰ã‚Œãªã„å ´åˆã¯è¡¨å±¤å½¢ã‚’ fallback ã«ã™ã‚‹
                if reading is None:
                    reading = surface
                # ä»¥å¾Œã®å‡¦ç†ã®ãŸã‚ã«æ–‡å­—åˆ—åŒ–ã—ã¦ãŠã
                reading = str(reading)

                # è¨˜å·ãƒ»è£œåŠ©è¨˜å·ã¯èª­ã¿ã‚’è¡¨å±¤å½¢ã«åˆã‚ã›ã‚‹
                tag = token.tag_
                if tag and "è¨˜å·" in tag:
                    reading = surface

                if surface == reading:
                    results.append({
                        "orig": surface,
                        "kana": reading,
                        "hira": surface,
                        "hepburn": surface,
                    })
                    continue

                # å˜ç´”ã«1æ–‡å­—ãšã¤å‡¦ç†
                if len(surface) == 1:
                    # 1æ–‡å­—ã®å ´åˆã¯ãã®ã¾ã¾
                    results.append({
                        "orig": surface,
                        "kana": reading,
                        "hira": self.kata_to_hira(reading),
                        "hepburn": katakana_to_hepburn(reading, use_macron=use_macron)
                    })
                else:
                    # è¤‡æ•°æ–‡å­—ã®å ´åˆã¯ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã«ä»»ã›ã‚‹ï¼ˆæ¼¢å­—/é€ã‚Šä»®åã®åˆ†å‰²ï¼‰
                    parts = self.split_kanji_okurigana(surface, reading, use_macron=use_macron)
                    # split_kanji_okurigana ã¯ã™ã§ã«è¾æ›¸ãƒªã‚¹ãƒˆï¼ˆorig/kana/hira/hepburnï¼‰ã‚’è¿”ã™ã®ã§ãã®ã¾ã¾è¿½åŠ 
                    results.extend(parts)

        return results

# --- ãƒ†ã‚¹ãƒˆ ---
if __name__ == "__main__":
    test_cases = [
        "ä½•",
        "ä½•ã§ã—ã‚‡ã†ã‹",
        "ä½•ãŒå¥½ãï¼Ÿ",
        "ä½•è‰²ãŒå¥½ãï¼Ÿ",
        "ä½•è‰²ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "ç¾ã—ã„èŠ±ã‚’è¦‹ã‚‹",
        "æ±äº¬ã«è¡Œã",
        "æ¼¢å­—ã¨ã‚«ã‚¿ã‚«ãƒŠã®æ··åœ¨",
        "ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼ã«è¡Œã",
        "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ã†",
        "ã‚·ã‚§ã‚¢ãƒã‚¦ã‚¹ã«ä½ã‚€",
        "ãƒ´ã‚¡ã‚¤ã‚ªãƒªãƒ³ã‚’å¼¾ã",
        "ã‚®ãƒ¥ã‚¦ãƒ‹ãƒ¥ã‚¦ã‚’é£²ã‚€",
        "ãƒ‹ãƒ¥ãƒ¼ãƒ¨ãƒ¼ã‚¯ã«è¡Œã",
        "ãƒ©ãƒ¼ãƒ¡ãƒ³ã‚’é£Ÿã¹ã‚‹",
        "ãƒãƒ§ã‚³ãƒ¬ãƒ¼ãƒˆãŒå¥½ã",
        "SessionIDã‚’å–å¾—ã™ã‚‹",
        "å–ã‚Šæ•¢ãˆãšæ¤œç´¢ã—ã¦ã¿ã‚‹",
        "è¦‹çŸ¥ã‚‰ã¬åœŸåœ°ã§å†’é™ºã™ã‚‹",
        "å½¼ã¯å„ªã‚ŒãŸã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™",
        " ".join(list("[]<>!@#$%^&*()_+-={}|\;:'\",.<>/?`~")),
        " ".join(list("ã€Œã€ï¼œï¼ï¼ï¼ ï¼ƒï¼„ï¼…ï¼¾ï¼†ï¼Šï¼ˆï¼‰ï¼¿ï¼‹ï¼ï¼ï½›ï½ï½œï¼¼ï¼›ï¼šï¼‡ï¼‚ï¼Œï¼ï¼ï¼Ÿï½€ï½")),
        " ".join(list("â™ªâ™«â™¬â™­â™®â™¯Â°â„ƒâ„‰â„–â„«Â®Â©â„¢âœ“âœ”âœ•âœ–â˜…â˜†â—‹â—â—â—‡â—†â–¡â– â–³â–²â–½â–¼â€»â†’â†â†‘â†“â†”ï¸â†•ï¸â‡„â‡…âˆâˆ´âˆµâˆ·â‰ªâ‰«â‰¦â‰§Â±Ã—Ã·â‰ â‰ˆâ‰¡âŠ‚âŠƒâŠ†âŠ‡âŠ„âŠ…âˆªâˆ©âˆˆâˆ‹âˆ…âˆ€âˆƒâˆ âŠ¥âŒ’âˆ‚âˆ‡âˆšâˆ«âˆ¬âˆ®âˆ‘âˆâˆ§âˆ¨Â¬â‡’â‡”âˆ€âˆƒâˆ âŠ¥âŒ’âˆ‚âˆ‡âˆšâˆ«âˆ¬âˆ®âˆ‘âˆ")),
        " ".join(list("ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ˜…ğŸ˜‚ğŸ¤£ğŸ˜ŠğŸ˜‡ğŸ™‚"))
    ]

    transliterator = Transliterator()
    for case in test_cases:
        print(transliterator.analyze(case))

    # nlp = spacy.load('ja_ginza')
    # for case in test_cases:
    #     doc = nlp(case)
    #     for sent in doc.sents:
    #         for token in sent:
    #             print(
    #                 token.orth_,
    #                 token.lemma_,
    #                 token.norm_,
    #                 token.morph.get("Reading"),
    #                 token.pos_,
    #                 token.morph.get("Inflection"),
    #                 token.tag_,
    #                 token.dep_,
    #                 token.head.i,
    #             )
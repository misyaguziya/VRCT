from sudachipy import tokenizer
from sudachipy import dictionary
try:
    from .transliteration_kana_to_hepburn import katakana_to_hepburn
except ImportError:
    from transliteration_kana_to_hepburn import katakana_to_hepburn

class Transliterator:
    def __init__(self):
        self.tokenizer_obj = dictionary.Dictionary().create()
        self.mode = tokenizer.Tokenizer.SplitMode.C

    @staticmethod
    def is_kanji(ch: str) -> bool:
        return '\u4e00' <= ch <= '\u9fff'

    @staticmethod
    def kata_to_hira(text: str) -> str:
        return "".join(
            chr(ord(c) - 0x60) if 'ã‚¡' <= c <= 'ãƒ³' else c
            for c in text
        )

    @staticmethod
    def split_kanji_okurigana(surface: str, reading_kana: str):
        """
        1èªã®è¡¨å±¤å½¢(surface)ã¨èª­ã¿(reading_kana)ã‚’
        [ {"orig":..., "kana":..., "hira":..., "hepburn":...}, ... ] ã«åˆ†å‰²
        """
        result = []

        # è¡¨å±¤ã‚’ã€Œæ¼¢å­—ãƒ–ãƒ­ãƒƒã‚¯ã€ã¨ã€Œéæ¼¢å­—ãƒ–ãƒ­ãƒƒã‚¯ã€ã«åˆ†å‰²
        buf = ""
        prev_is_kanji = None
        blocks = []
        for ch in surface:
            now_is_kanji = Transliterator.is_kanji(ch)
            if prev_is_kanji is None or now_is_kanji == prev_is_kanji:
                buf += ch
            else:
                blocks.append((prev_is_kanji, buf))
                buf = ch
            prev_is_kanji = now_is_kanji
        if buf:
            blocks.append((prev_is_kanji, buf))

        # èª­ã¿ã‚’åˆ†é…
        kana_left = reading_kana
        for i, (is_kan, part) in enumerate(blocks):
            if is_kan:
                # æ¼¢å­—ãƒ–ãƒ­ãƒƒã‚¯ã®å‡¦ç†
                if len(blocks) == 1:
                    # å˜ä¸€ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆå…¨ã¦æ¼¢å­—ï¼‰ã®å ´åˆ
                    kana_for_kan = kana_left
                elif i == len(blocks) - 1:
                    # æœ€å¾Œã®ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆæ¼¢å­—ï¼‰ã®å ´åˆ
                    kana_for_kan = kana_left
                else:
                    # ä¸­é–“ã®æ¼¢å­—ãƒ–ãƒ­ãƒƒã‚¯ã®å ´åˆ
                    # å¾Œç¶šã®éæ¼¢å­—ãƒ–ãƒ­ãƒƒã‚¯ã®æ–‡å­—æ•°ã‚’è¨ˆç®—
                    remaining_non_kanji = sum(len(p) for is_k, p in blocks[i+1:] if not is_k)
                    if remaining_non_kanji > 0 and len(kana_left) > remaining_non_kanji:
                        kana_for_kan = kana_left[:-remaining_non_kanji]
                    else:
                        # æ¼¢å­—1æ–‡å­—ã‚ãŸã‚Šæœ€ä½1æ–‡å­—ã®èª­ã¿ã‚’å‰²ã‚Šå½“ã¦
                        min_kana = len(part)
                        kana_for_kan = kana_left[:max(min_kana, len(kana_left) - remaining_non_kanji)]
                
                # ç©ºã®èª­ã¿ã‚’é¿ã‘ã‚‹
                if not kana_for_kan and kana_left:
                    kana_for_kan = kana_left[:1]
                
                result.append(
                    {
                        "orig": part,
                        "kana": kana_for_kan,
                        "hira": Transliterator.kata_to_hira(kana_for_kan),
                        "hepburn": katakana_to_hepburn(kana_for_kan, use_macron=True)
                    }
                )
                kana_left = kana_left[len(kana_for_kan):]
            else:
                # éæ¼¢å­—éƒ¨åˆ†ï¼ˆé€ã‚Šä»®åãªã©ï¼‰
                kana_for_okuri = kana_left[:len(part)]
                result.append(
                    {
                        "orig": part,
                        "kana": kana_for_okuri,
                        "hira": Transliterator.kata_to_hira(kana_for_okuri),
                        "hepburn": katakana_to_hepburn(kana_for_okuri, use_macron=True)
                    }
                )
                kana_left = kana_left[len(kana_for_okuri):]

        return result

    def analyze(self, text: str, use_macron: bool = True):
        tokens = self.tokenizer_obj.tokenize(text, self.mode)

        results = []
        for t in tokens:
            surface = t.surface()
            reading = t.reading_form()
            pos = t.part_of_speech()
            print("surface:", surface, " reading:", reading, " pos:", pos)

            if pos and pos[0] in ["è¨˜å·", "è£œåŠ©è¨˜å·"]:
                reading = surface

            if surface == reading:
                results.append({
                    "orig": surface,
                    "kana": reading,
                    "hira": surface,
                    "hepburn": surface,
                })
                continue

            # å˜ç´”ã«1æ–‡å­—ãšã¤å‡¦ç†
            if len(surface) == 1:
                # 1æ–‡å­—ã®å ´åˆã¯ãã®ã¾ã¾
                results.append({
                    "orig": surface,
                    "kana": reading,
                    "hira": self.kata_to_hira(reading),
                    "hepburn": katakana_to_hepburn(reading, use_macron=use_macron)
                })
            else:
                # è¤‡æ•°æ–‡å­—ã®å ´åˆã¯æ–‡å­—ç¨®åˆ¥ã§åˆ†å‰²
                i = 0
                reading_pos = 0
                
                while i < len(surface):
                    char = surface[i]
                    
                    if self.is_kanji(char):
                        # æ¼¢å­—ã®å ´åˆã€é€£ç¶šã™ã‚‹æ¼¢å­—ã‚’ã¾ã¨ã‚ã¦å‡¦ç†
                        kanji_block = ""
                        while i < len(surface) and self.is_kanji(surface[i]):
                            kanji_block += surface[i]
                            i += 1
                        
                        # æ¼¢å­—ãƒ–ãƒ­ãƒƒã‚¯ã®èª­ã¿ã‚’æ¨å®š
                        if i < len(surface):
                            # å¾Œã«æ–‡å­—ãŒã‚ã‚‹å ´åˆã€é€ã‚Šä»®åã‚’è€ƒæ…®
                            remaining_chars = len(surface) - i
                            kanji_reading = reading[reading_pos:-remaining_chars] if remaining_chars > 0 else reading[reading_pos:]
                        else:
                            # æœ€å¾Œã®æ¼¢å­—ãƒ–ãƒ­ãƒƒã‚¯ã®å ´åˆ
                            kanji_reading = reading[reading_pos:]

                        # ç©ºã®èª­ã¿ã‚’é¿ã‘ã‚‹
                        if not kanji_reading and reading_pos < len(reading):
                            kanji_reading = reading[reading_pos:]
                        if not kanji_reading and kanji_block:
                            # èª­ã¿ãŒç©ºã ãŒæ¼¢å­—ãƒ–ãƒ­ãƒƒã‚¯ãŒã‚ã‚‹å ´åˆã€æ®‹ã‚Šã®èª­ã¿ã‚’å…¨ã¦å‰²ã‚Šå½“ã¦ã‚‹
                            kanji_reading = reading[reading_pos:]

                        # reading_posã®æ›´æ–°ã‚’æ­£ç¢ºã«è¡Œã†ãŸã‚ã«ã€å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸèª­ã¿ã®é•·ã•ã‚’ãƒã‚§ãƒƒã‚¯
                        len_allocated_reading = len(kanji_reading)
                        if reading_pos + len_allocated_reading > len(reading):
                            len_allocated_reading = len(reading) - reading_pos

                        results.append({
                            "orig": kanji_block,
                            "kana": kanji_reading,
                            "hira": self.kata_to_hira(kanji_reading),
                            "hepburn": katakana_to_hepburn(kanji_reading, use_macron=use_macron)
                        })
                        reading_pos += len_allocated_reading
                    else:
                        # éæ¼¢å­—ã®å ´åˆ
                        non_kanji_block = ""
                        while i < len(surface) and not self.is_kanji(surface[i]):
                            non_kanji_block += surface[i]
                            i += 1

                        # éæ¼¢å­—éƒ¨åˆ†ã®èª­ã¿ï¼ˆé€šå¸¸ã¯æ–‡å­—æ•°åˆ†ã€ã¾ãŸã¯æ®‹ã‚Šã®èª­ã¿ã®åˆ†ã ã‘ï¼‰
                        len_block = len(non_kanji_block)
                        non_kanji_reading = reading[reading_pos:reading_pos + len_block]

                        # å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸèª­ã¿ã®é•·ã•
                        len_allocated_reading = len(non_kanji_reading)

                        results.append({
                            "orig": non_kanji_block,
                            "kana": non_kanji_reading,
                            "hira": self.kata_to_hira(non_kanji_reading),
                            "hepburn": katakana_to_hepburn(non_kanji_reading, use_macron=use_macron)
                        })
                        reading_pos += len_allocated_reading

        return results

# --- ãƒ†ã‚¹ãƒˆ ---
if __name__ == "__main__":
    test_cases = [
        "ç¾ã—ã„èŠ±ã‚’è¦‹ã‚‹",
        "æ±äº¬ã«è¡Œã",
        "æ¼¢å­—ã¨ã‚«ã‚¿ã‚«ãƒŠã®æ··åœ¨",
        "ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼ã«è¡Œã",
        "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ã†",
        "ã‚·ã‚§ã‚¢ãƒã‚¦ã‚¹ã«ä½ã‚€",
        "ãƒ´ã‚¡ã‚¤ã‚ªãƒªãƒ³ã‚’å¼¾ã",
        "ã‚®ãƒ¥ã‚¦ãƒ‹ãƒ¥ã‚¦ã‚’é£²ã‚€",
        "ãƒ‹ãƒ¥ãƒ¼ãƒ¨ãƒ¼ã‚¯ã«è¡Œã",
        "ãƒ©ãƒ¼ãƒ¡ãƒ³ã‚’é£Ÿã¹ã‚‹",
        "ãƒãƒ§ã‚³ãƒ¬ãƒ¼ãƒˆãŒå¥½ã",
        "SessionIDã‚’å–å¾—ã™ã‚‹",
        "å–ã‚Šæ•¢ãˆãšæ¤œç´¢ã—ã¦ã¿ã‚‹",
        "è¦‹çŸ¥ã‚‰ã¬åœŸåœ°ã§å†’é™ºã™ã‚‹",
        "å½¼ã¯å„ªã‚ŒãŸã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™",
        " ".join(list("[]<>!@#$%^&*()_+-={}|\;:'\",.<>/?`~")),
        " ".join(list("ã€Œã€ï¼œï¼ï¼ï¼ ï¼ƒï¼„ï¼…ï¼¾ï¼†ï¼Šï¼ˆï¼‰ï¼¿ï¼‹ï¼ï¼ï½›ï½ï½œï¼¼ï¼›ï¼šï¼‡ï¼‚ï¼Œï¼ï¼ï¼Ÿï½€ï½")),
        " ".join(list("â™ªâ™«â™¬â™­â™®â™¯Â°â„ƒâ„‰â„–â„«Â®Â©â„¢âœ“âœ”âœ•âœ–â˜…â˜†â—‹â—â—â—‡â—†â–¡â– â–³â–²â–½â–¼â€»â†’â†â†‘â†“â†”ï¸â†•ï¸â‡„â‡…âˆâˆ´âˆµâˆ·â‰ªâ‰«â‰¦â‰§Â±Ã—Ã·â‰ â‰ˆâ‰¡âŠ‚âŠƒâŠ†âŠ‡âŠ„âŠ…âˆªâˆ©âˆˆâˆ‹âˆ…âˆ€âˆƒâˆ âŠ¥âŒ’âˆ‚âˆ‡âˆšâˆ«âˆ¬âˆ®âˆ‘âˆâˆ§âˆ¨Â¬â‡’â‡”âˆ€âˆƒâˆ âŠ¥âŒ’âˆ‚âˆ‡âˆšâˆ«âˆ¬âˆ®âˆ‘âˆ")),
        " ".join(list("ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ˜…ğŸ˜‚ğŸ¤£ğŸ˜ŠğŸ˜‡ğŸ™‚"))
    ]

    transliterator = Transliterator()
    for case in test_cases:
        print(transliterator.analyze(case))